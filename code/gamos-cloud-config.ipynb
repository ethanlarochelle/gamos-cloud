{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud configuration (AWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import base64\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "input_dir = '../data/'\n",
    "output_dir = '../results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boto3 configuration\n",
    "An AWS account must be created and configured before begining with these examples. This involves \n",
    "* [registering with AWS, and then](https://aws.amazon.com/)\n",
    "* [creating a user with API access](https://console.aws.amazon.com/iam/home#/users)\n",
    "  * `Add User` with `Programmatic access` \n",
    "  * Give `AdminPowerUser` or other appropriate permissions\n",
    "* Copy the `access key ID`  and `secret access key`\n",
    "* Create the file ~/.aws/config (where ~ is your HOME directory) and add youy keys ([More details here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html)):\n",
    "\n",
    "```\n",
    "[default]\n",
    "aws_access_key_id=foo\n",
    "aws_secret_access_key=bar\n",
    "```\n",
    "\n",
    "Once this file is in place with valid credentials, you should be able to connect to AWS using the Boto3 AWS Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by listing users in your account\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "paginator = iam.get_paginator('list_users')\n",
    "for response in paginator.paginate():\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Creating an EC2 instance\n",
    "This is a remote server that can be turned on and off as needed.\n",
    "These servers are based on an Amazon Machine Image (AMI) and also need a key pair for remote connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Boto3 EC2 resources\n",
    "ec2 = boto3.resource('ec2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_name = 'EC2_GAMOS6_1_example2'\n",
    "# Create a key-pair for accessing future EC2 instances\n",
    "outfile = open(os.path.join(output_dir, '{}.pem'.format(key_name)), 'w')\n",
    "key_pair = ec2.create_key_pair(KeyName=key_name)\n",
    "KeyPairOut= str(key_pair.key_material)\n",
    "outfile.write(KeyPairOut)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EC2 instance\n",
    "# The Ebs dictionary is optional\n",
    "# The Image ID will change. Herewe're using the latest Ubuntu 18.04LTS, but 20.04LTS will be released soon\n",
    "#  - For an updated list check: https://cloud-images.ubuntu.com/locator/ec2/\n",
    "# The intance type can be anything listed here: https://aws.amazon.com/ec2/instance-types/\n",
    "# The SecurityGroupsIds\n",
    "new_instances = ec2.create_instances(\n",
    "    BlockDeviceMappings=[\n",
    "        {\n",
    "            'DeviceName': '/dev/sda1',\n",
    "            'Ebs': {\n",
    "                'DeleteOnTermination': True,\n",
    "                'VolumeSize': 20, # GB\n",
    "                'VolumeType': 'standard',\n",
    "                'Encrypted': False,\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    ImageId='ami-0367b500fdcac0edc', # this value will change based on the latest release and region\n",
    "    MinCount=1, \n",
    "    MaxCount=1,\n",
    "    KeyName=key_name,\n",
    "    InstanceType=\"t3.large\",\n",
    "    #SecurityGroupIds=[security_group_id], # See below: This defines a the permissions of how traffic can flow in and out\n",
    "    TagSpecifications=[{'ResourceType':'instance',\n",
    "                        'Tags': [{\"Key\": \"Name\",\n",
    "                                  \"Value\": \"Run GAMOS 6_1\"}]}],\n",
    "    DryRun=False\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EC2 Security Groups (optional)\n",
    "Security groups are used to define ingress and egress permissions from EC2 instances\n",
    "Ports, protocols, and IP ranges can be used to configure these parameters\n",
    "For example, traffic to websites normally travels to port 80 (http) or 433 (https), while terminal connections are commonly made over port 21 (FTP) of 22 (SSH).\n",
    "\n",
    "The IP range can be defined using [CIDR notation](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing), where 0.0.0.0/0 is all IPv4 addresses on the internet, [129.170.0.0/16](https://ipinfo.io/AS10755/129.170.0.0/16) is the IPv4 address space controlled by Dartmouth College, and a.b.c.d/32 would be a single public IP. \n",
    "\n",
    "By executing the code block below, and then running the block above using the `security_group_id` variable, the EC2 instance will limit ingress and egress based on the paramters in the security group. So, for ingress requests the system will only respond on the given port from requests originating in the IP range specified. The connection will also still require the SSH key created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates 2 security groups\n",
    "#  1) Only allows SSH access from Dartmouth IP space,\n",
    "#  2) Allows all connections from within security group \n",
    "# Both allow all egress from the EC2 instance\n",
    "ec2_client = boto3.client('ec2')\n",
    "\n",
    "response = ec2_client.describe_vpcs()\n",
    "vpc_id = response.get('Vpcs', [{}])[0].get('VpcId', '')\n",
    "\n",
    "\n",
    "try:\n",
    "    response = ec2_client.create_security_group(GroupName='Dartmouth_ssh4',\n",
    "                                         Description='SSH from Dartmouth IP space (API)',\n",
    "                                         VpcId=vpc_id)\n",
    "    dartmouth_security_group_id = response['GroupId']\n",
    "    print('Security Group Created %s in vpc %s.' % (dartmouth_security_group_id, vpc_id))\n",
    "\n",
    "    data = ec2_client.authorize_security_group_ingress(\n",
    "        GroupId=dartmouth_security_group_id,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 22,\n",
    "             'ToPort': 22,\n",
    "             'IpRanges': [{'CidrIp': '129.170.0.0/16'}]}\n",
    "        ])\n",
    "    print('Ingress Successfully Set %s' % data)\n",
    "except ClientError as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    response = ec2_client.create_security_group(GroupName='DefaultAPI4',\n",
    "                                         Description='Default (API)',\n",
    "                                         VpcId=vpc_id)\n",
    "    default_security_group_id = response['GroupId']\n",
    "    print('Security Group Created %s in vpc %s.' % (default_security_group_id, vpc_id))\n",
    "\n",
    "    data = ec2_client.authorize_security_group_ingress(\n",
    "        GroupId=default_security_group_id,\n",
    "        IpPermissions=[\n",
    "            {\n",
    "             'IpProtocol': '-1',\n",
    "            'UserIdGroupPairs': [{'GroupId': default_security_group_id}]\n",
    "            }\n",
    "        ])\n",
    "    print('Ingress Successfully Set %s' % data)\n",
    "except ClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List EC2 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances = ec2_client.describe_instances()\n",
    "for instances in all_instances['Reservations']:\n",
    "    for instance in instances['Instances']:\n",
    "        print(instance['InstanceId'], instance['InstanceType'], instance['ImageId'], instance['KeyName'],instance['PublicDnsName'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start/stop/terminate EC2 instance\n",
    "The code below allows the user to start, stop, or terminate an instance based on the `InstanceId`\n",
    "The code can be un-commented by selecting the desired lines and pressing both `ctl  /`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start instance with given ID\n",
    "# ec2_client.start_instances(\n",
    "#     InstanceIds=[<instance_id>],\n",
    "#     DryRun=False\n",
    "# )\n",
    "\n",
    "# Stop instance with given ID\n",
    "# This allows the instance to shutdown and be re-used at a later time\n",
    "#ec2.stop_instances(\n",
    "#    InstanceIds=[<instance_id>],\n",
    "#    DryRun=True\n",
    "#)\n",
    "\n",
    "# Terminate instance with given ID\n",
    "# This deletes the EC2 instance so it will need to be re-created if needed in the future\n",
    "# ec2.terminate_instances(\n",
    "#     InstanceIds=[<instance_id>],\n",
    "#     DryRun=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to EC2 on the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an EC2 instance is created and available, this key can be used to access the instance from the terminal, where <`key_name`> is replaced with the variable defined earlier, and `<hostname>.<region>` is the `instance['PublicDnsName']` provided when the EC2 instances were listed. \n",
    "\n",
    "The user will be `ubuntu` for Ubuntu AMIs, or `ec2-user` ECS-optimized AMIs (used for AWS Batch containers).\n",
    "\n",
    "```\n",
    "chmod 600 ../results/<key_name>.pem\n",
    "ssh -i ../results/<key_name>.pem  ubuntu@<hostname>.<region>.compute.amazonaws.com\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Docker on EC2\n",
    "On the EC2 instance enter the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal\n",
    "```\n",
    "mkdir Downloads\n",
    "cd Downloads\n",
    "wget \"http://fismed.ciemat.es/GAMOS/download/GAMOS.6.1.0/download_scripts.sh\"\n",
    "chmod 755 download_scripts.sh\n",
    "./download_scripts.sh\n",
    "mkdir ~/docker_dev\n",
    "cd ~/docker_dev\n",
    "cp ~/Downloads/scripts/* .\n",
    "sudo apt-get update\n",
    "sudo apt-get install docker.io\n",
    "vim Dockerfile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vim Dockerfile\n",
    "This is descibed in the corresponding paper. Briefly, this file describes what is needed to configure a GAMOS container\n",
    "```\n",
    "FROM ubuntu:18.04\n",
    "  \n",
    "WORKDIR /docker_gamos\n",
    "\n",
    "\n",
    "ADD . /docker_gamos\n",
    "ADD fetch_and_run.sh /usr/local/bin/fetch_and_run.sh\n",
    "\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y --no-install-recommends apt-utils\n",
    "# Add sudo permissions for admin user\n",
    "RUN apt-get install -y sudo\n",
    "RUN adduser --disabled-password --gecos '' docker\n",
    "RUN adduser docker sudo\n",
    "RUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n",
    "USER docker\n",
    "RUN ./installMissingPackages.Docker.Ubuntu.18.04.sh\n",
    "RUN sudo ./installGamos.sh /docker_gamos/gamos\n",
    "RUN source gamos/GAMOS.6.1.0/config/confgamos.sh\n",
    "\n",
    "ENTRYPOINT [\"/docker_gamos/fetch_and_run.sh\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vim installMissingPackages.Docker.Ubuntu.18.04.sh\n",
    "Not all of these packages are needed, but a number were added to the generic Ubuntu.18.04 file to reduce errors when comiling\n",
    "```\n",
    "sudo apt-get -y install curl wget unzip git vim\n",
    "sudo apt-get -y install g++\n",
    "sudo apt-get -y install dpkg-dev\n",
    "sudo apt-get -y install binutils zlibc zopfli\n",
    "sudo apt-get -y install libx11-dev\n",
    "sudo apt-get -y install libxpm-dev\n",
    "sudo apt-get -y install libpng12-dev libfreetype6-dev\n",
    "sudo apt-get -y install libxft-dev\n",
    "sudo apt-get -y install libxext-dev\n",
    "sudo apt-get -y install freeglut3-dev libglfw3-dev libglfw3 libglu1-mesa-dev mesa-common-dev\n",
    "sudo apt-get -y install libxmu-dev\n",
    "sudo apt-get -y install libxi-dev\n",
    "sudo apt-get -y install libtiff-dev\n",
    "sudo apt-get -y install cmake\n",
    "sudo apt-get -y install libafterimage-dev\n",
    "sudo apt-get -y install build-essential\n",
    "sudo apt-get -y install libjpeg8-dev\n",
    "sudo apt-get -y install libtiff5-dev\n",
    "sudo apt-get -y install python\n",
    "sudo apt-get -y install python-dev\n",
    "sudo apt-get -y install python-numpy-dev\n",
    "sudo apt-get -y install libtool\n",
    "sudo apt-get -y install gfortran libfftw3-dev\n",
    "sudo apt-get -y install libboost-tools-dev libboost-thread1.62-dev magics++\n",
    "sudo apt-get -y install libgsl0-dev\n",
    "sudo apt-get -y install libcxxtools-dev\n",
    "sudo apt-get -y install librte-pmd-failsafe17.11\n",
    "sudo curl \"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\" -o \"awscli-bundle.zip\"\n",
    "sudo unzip awscli-bundle.zip\n",
    "sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vim fetch_and_run.sh\n",
    "Taken from the [AWS Batch blog](https://aws.amazon.com/blogs/compute/creating-a-simple-fetch-and-run-aws-batch-job/)\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Copyright 2013-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the\n",
    "# License. A copy of the License is located at\n",
    "#\n",
    "# http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"LICENSE.txt\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES\n",
    "# OR CONDITIONS OF ANY KIND, express or implied. See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# This script can help you download and run a script from S3 using aws-cli.\n",
    "# It can also download a zip file from S3 and run a script from inside.\n",
    "# See below for usage instructions.\n",
    "\n",
    "PATH=\"/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin\"\n",
    "BASENAME=\"${0##*/}\"\n",
    "\n",
    "usage () {\n",
    "  if [ \"${#@}\" -ne 0 ]; then\n",
    "    echo \"* ${*}\"\n",
    "    echo\n",
    "  fi\n",
    "  cat <<ENDUSAGE\n",
    "Usage:\n",
    "\n",
    "export BATCH_FILE_TYPE=\"script\"\n",
    "export BATCH_FILE_S3_URL=\"s3://my-bucket/my-script\"\n",
    "${BASENAME} script-from-s3 [ <script arguments> ]\n",
    "\n",
    "  - or -\n",
    "\n",
    "export BATCH_FILE_TYPE=\"zip\"\n",
    "export BATCH_FILE_S3_URL=\"s3://my-bucket/my-zip\"\n",
    "${BASENAME} script-from-zip [ <script arguments> ]\n",
    "ENDUSAGE\n",
    "\n",
    "  exit 2\n",
    "}\n",
    "\n",
    "# Standard function to print an error and exit with a failing return code\n",
    "error_exit () {\n",
    "  echo \"${BASENAME} - ${1}\" >&2\n",
    "  exit 1\n",
    "}\n",
    "\n",
    "# Check what environment variables are set\n",
    "if [ -z \"${BATCH_FILE_TYPE}\" ]; then\n",
    "  usage \"BATCH_FILE_TYPE not set, unable to determine type (zip/script) of URL ${BATCH_FILE_S3_URL}\"\n",
    "fi\n",
    "\n",
    "if [ -z \"${BATCH_FILE_S3_URL}\" ]; then\n",
    "  usage \"BATCH_FILE_S3_URL not set. No object to download.\"\n",
    "fi\n",
    "\n",
    "scheme=\"$(echo \"${BATCH_FILE_S3_URL}\" | cut -d: -f1)\"\n",
    "if [ \"${scheme}\" != \"s3\" ]; then\n",
    "  usage \"BATCH_FILE_S3_URL must be for an S3 object; expecting URL starting with s3://\"\n",
    "fi\n",
    "\n",
    "# Check that necessary programs are available\n",
    "which aws >/dev/null 2>&1 || error_exit \"Unable to find AWS CLI executable.\"\n",
    "which unzip >/dev/null 2>&1 || error_exit \"Unable to find unzip executable.\"\n",
    "\n",
    "# Create a temporary directory to hold the downloaded contents, and make sure\n",
    "# it's removed later, unless the user set KEEP_BATCH_FILE_CONTENTS.\n",
    "cleanup () {\n",
    "   if [ -z \"${KEEP_BATCH_FILE_CONTENTS}\" ] \\\n",
    "     && [ -n \"${TMPDIR}\" ] \\\n",
    "     && [ \"${TMPDIR}\" != \"/\" ]; then\n",
    "      rm -r \"${TMPDIR}\"\n",
    "   fi\n",
    "}\n",
    "trap 'cleanup' EXIT HUP INT QUIT TERM\n",
    "# mktemp arguments are not very portable.  We make a temporary directory with\n",
    "# portable arguments, then use a consistent filename within.\n",
    "TMPDIR=\"$(mktemp -d -t tmp.XXXXXXXXX)\" || error_exit \"Failed to create temp directory.\"\n",
    "TMPFILE=\"${TMPDIR}/batch-file-temp\"\n",
    "install -m 0600 /dev/null \"${TMPFILE}\" || error_exit \"Failed to create temp file.\"\n",
    "\n",
    "# Fetch and run a script\n",
    "fetch_and_run_script () {\n",
    "  # Create a temporary file and download the script\n",
    "  aws s3 cp \"${BATCH_FILE_S3_URL}\" - > \"${TMPFILE}\" || error_exit \"Failed to download S3 script.\"\n",
    "\n",
    "  # Make the temporary file executable and run it with any given arguments\n",
    "  local script=\"./${1}\"; shift\n",
    "  chmod u+x \"${TMPFILE}\" || error_exit \"Failed to chmod script.\"\n",
    "  exec ${TMPFILE} \"${@}\" || error_exit \"Failed to execute script.\"\n",
    "}\n",
    "\n",
    "# Download a zip and run a specified script from inside\n",
    "fetch_and_run_zip () {\n",
    "  # Create a temporary file and download the zip file\n",
    "  aws s3 cp \"${BATCH_FILE_S3_URL}\" - > \"${TMPFILE}\" || error_exit \"Failed to download S3 zip file from ${BATCH_FILE_S3_URL}\"\n",
    "\n",
    "  # Create a temporary directory and unpack the zip file\n",
    "  cd \"${TMPDIR}\" || error_exit \"Unable to cd to temporary directory.\"\n",
    "  unzip -q \"${TMPFILE}\" || error_exit \"Failed to unpack zip file.\"\n",
    "\n",
    "  # Use first argument as script name and pass the rest to the script\n",
    "  local script=\"./${1}\"; shift\n",
    "  [ -r \"${script}\" ] || error_exit \"Did not find specified script '${script}' in zip from ${BATCH_FILE_S3_URL}\"\n",
    "  chmod u+x \"${script}\" || error_exit \"Failed to chmod script.\"\n",
    "  exec \"${script}\" \"${@}\" || error_exit \" Failed to execute script.\"\n",
    "}\n",
    "\n",
    "# Main - dispatch user request to appropriate function\n",
    "case ${BATCH_FILE_TYPE} in\n",
    "  zip)\n",
    "    if [ ${#@} -eq 0 ]; then\n",
    "      usage \"zip format requires at least one argument - the script to run from inside\"\n",
    "    fi\n",
    "    fetch_and_run_zip \"${@}\"\n",
    "    ;;\n",
    "\n",
    "  script)\n",
    "    fetch_and_run_script \"${@}\"\n",
    "    ;;\n",
    "\n",
    "  *)\n",
    "    usage \"Unsupported value for BATCH_FILE_TYPE. Expected (zip/script).\"\n",
    "    ;;\n",
    "esac\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vim getGamosFiles.sh\n",
    "This modifies the files provided by the GAMOS install scripts to change the location GAMOS is installed from. It also adds optical examples.\n",
    "\n",
    "After the following\n",
    "```bash\n",
    "echo Installing GAMOS version $GAMOS_VER...\n",
    "tar zxf $GAMOSFILES/GAMOS.$GAMOS_VER.tgz\n",
    "```\n",
    "\n",
    "Add these lines:\n",
    "\n",
    "```bash\n",
    "echo Adding GAMOS version with Tissue Optics plugin\n",
    "mkdir $HOME/backup\n",
    "mv $GAMOSFILES/GAMOS.$GAMOS_VER/source/GamosCore $HOME/backup\n",
    "wget -N https://github.com/ethanlarochelle/GamosCore/archive/6_1.zip\n",
    "unzip 6_1.zip\n",
    "mv GamosCore-6_1 GamosCore\n",
    "mv GamosCore $GAMOSFILES/GAMOS.$GAMOS_VER/source/\n",
    "rm 6_1.zip\n",
    "echo Adding Tissue Optics tutorials\n",
    "wget -N https://github.com/ethanlarochelle/GAMOS_examples/archive/master.zip\n",
    "unzip master.zip\n",
    "mv GAMOS_examples-master TissueOptics\n",
    "mv TissueOptics $GAMOSFILES/GAMOS.$GAMOS_VER/tutorials/\n",
    "rm master.zip\n",
    "```\n",
    "Which are followed by:\n",
    "```bash\n",
    "cd  $GAMOSFILES/GAMOS.$GAMOS_VER/data\n",
    "tar zxf initialSeeds.tgz\n",
    "cd -\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker image\n",
    "Back at the terminal run the following:\n",
    "```console\n",
    "cd ~/docker_dev\n",
    "chmod 755 installMissingPackages.Docker.Ubuntu.18.04.sh \n",
    "chmod 755 fetch_and_run.sh \n",
    "sudo systemctl start docker\n",
    "sudo systemctl enable docker\n",
    "sudo docker system prune -a\n",
    "sudo docker build -t gamos_6_1_tissue_optics .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screen (Optional)\n",
    "\n",
    "Before docker installation, use screen to allow the installation to contnue if the network connection is dropped\n",
    "``` console\n",
    "screen -S docker_install\n",
    "screen -ls # returns 5-digit id of screen session XXXXX\n",
    "screen -r XXXXX\n",
    "sudo docker build -t gamos_6_1_tissue_optics .\n",
    "[ctl]+a d # to dettach screen session. Returns: [detached from 20449.docker_install]\n",
    "exit\n",
    "```\n",
    "\n",
    "[ctl]+a K # Kills screen session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Docker Container (Optional)\n",
    "Start and check container status after build\n",
    "```console\n",
    "sudo docker run -d -it --entrypoint \"/bin/bash\" --name=<name> gamos_6_1_tissue_optics\n",
    "sudo docker exec -it <name> /bin/bash\n",
    "```\n",
    "On Docker command line: \n",
    "```console\n",
    "source gamos/GAMOS.6.1.0/config/confgamos.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Container registry (ECR)\n",
    "This is where the container image will be stored within AWS. Alternatively, it could be stored on DockerHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_client = boto3.client('ecr')\n",
    "ecr_name = 'monte-carlo/gamos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ECR repository, or if it is alread created get the details\n",
    "try:\n",
    "    ecr_repository = ecr_client.create_repository(\n",
    "                        repositoryName=ecr_name)\n",
    "except ClientError as e:\n",
    "    ecr_repository = {}\n",
    "    ecr_repository['repository'] = ecr_client.describe_repositories(repositoryNames=['monte-carlo/gamos'])['repositories'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoded Token to allow access to push containter for 12 hours\n",
    "# *** Only need to run this if AWS CLI not installed on Docker build system ***\n",
    "encoded_token = ecr_client.get_authorization_token()['authorizationData'][0]['authorizationToken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Only need to run this if AWS CLI not installed on Docker build system ***\n",
    "# Decode token\n",
    "decoded_token = base64.b64decode(encoded_token).decode('UTF-8')\n",
    "decoded_token_no_user = decoded_token.split('AWS:')[1]\n",
    "# Copy token to clipboard (returns 0 on success)\n",
    "os.system(\"echo '{}' | pbcopy\".format(decoded_token_no_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display URI to use in cell below\n",
    "ecr_repository['repository']['repositoryUri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing container to repository\n",
    "Replace `<ecr_repository['repository']['repositoryUri']>` with the output of the above code cell\n",
    "\n",
    "On build EC2 insance\n",
    "\n",
    " If AWS command line interface is installed on system building docker image use the following (change region as appropriate):\n",
    " ```console\n",
    " aws ecr get-login --region us-east-2\n",
    " ```\n",
    " Else, if building on a system without AWS credentials, like a new EC2 instance, use the following with the copied token above. Replace the URI with your Repository URI.\n",
    " \n",
    "```console\n",
    "sudo docker login -u AWS  <ecr_repository['repository']['repositoryUri']>\n",
    "<paste decoded token, above>\n",
    "```\n",
    "\n",
    "Then run the following to tag and push the container image. Replace the link as appropriate for your configuration.\n",
    "```console\n",
    "sudo docker tag gamos_6_1_tissue_optics:latest <ecr_repository['repository']['repositoryUri']>:latest\n",
    "sudo docker push <ecr_repository['repository']['repositoryUri']>:latest\n",
    "```\n",
    "\n",
    "At this point EC2 instance used to create container can be stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if container image is registered\n",
    "container_images = ecr_client.describe_images(\n",
    "    repositoryName='monte-carlo/gamos')\n",
    "container_images['imageDetails']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy simulation template to S3\n",
    "Template simulations are created in a single directory. This directory has everything needed to run the simulation and accept argument inputs. The folder is compressed and placed in an AWS storage bucket using S3. AWS Batch will later be configured to look in this bucket for a zip file, decompress it and run a shell script with given argument inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client=boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6MV Fluence detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress simulation template\n",
    "os.path.join(input_dir, '../data', '6MV_Fluence')\n",
    "run_zip_name_f= 'gamos-ptg4-6mv-fluence'\n",
    "archive_file_f = shutil.make_archive(run_zip_name_f, 'zip', os.path.join(input_dir, '6MV_Fluence'))\n",
    "shutil.move(archive_file_f, input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_response_f = s3_client.create_bucket(\n",
    "    Bucket=run_zip_name_f,\n",
    "    CreateBucketConfiguration={\n",
    "        'LocationConstraint': 'us-east-2'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource.meta.client.upload_file(os.path.join(input_dir, '{}.zip'.format(run_zip_name_f)), run_zip_name_f, '{}.zip'.format(run_zip_name_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6MV No fluence detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress simulation template\n",
    "os.path.join(input_dir, '../data', '6MV_NoFluence')\n",
    "run_zip_name_nf = 'gamos-ptg4-6mv-nofluence'\n",
    "archive_file = shutil.make_archive(run_zip_name_nf, 'zip', os.path.join(input_dir, '6MV_NoFluence'))\n",
    "shutil.move(archive_file, input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_response = s3_client.create_bucket(\n",
    "    Bucket=run_zip_name_nf,\n",
    "    CreateBucketConfiguration={\n",
    "        'LocationConstraint': 'us-east-2'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource.meta.client.upload_file(os.path.join(input_dir, '{}.zip'.format(run_zip_name_nf)), run_zip_name_nf, '{}.zip'.format(run_zip_name_nf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Batch\n",
    "AWS Batch is used to automatically organize containers on EC2 instances and execute multiple tasks in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch permissions\n",
    "Identity access management (IAM) permissions need to be set to allow certain tasks to be automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job_role_policy = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Statement': [{'Sid': '',\n",
    "    'Effect': 'Allow',\n",
    "    'Principal': {'Service': 'ecs-tasks.amazonaws.com'},\n",
    "    'Action': 'sts:AssumeRole'}]\n",
    "}\n",
    "try:\n",
    "    job_role_response = iam.create_role(\n",
    "        RoleName='Batch_IAM_Role',\n",
    "        AssumeRolePolicyDocument=json.dumps(batch_job_role_policy),\n",
    "        Description='IAM Role for running GAMOS Batch runs',\n",
    "        MaxSessionDuration=14400)\n",
    "except ClientError as e:\n",
    "    job_role_response = iam.get_role(RoleName='Batch_IAM_Role')\n",
    "    \n",
    "# EC2 access\n",
    "iam.attach_role_policy(\n",
    "    RoleName=job_role_response['Role']['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonEC2FullAccess')\n",
    "\n",
    "# S3 Access\n",
    "iam.attach_role_policy(\n",
    "    RoleName=job_role_response['Role']['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess')\n",
    "\n",
    "# API Gateway access\n",
    "iam.attach_role_policy(\n",
    "    RoleName=job_role_response['Role']['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonAPIGatewayAdministrator')\n",
    "\n",
    "# EC2 Conatiner Service\n",
    "iam.attach_role_policy(\n",
    "    RoleName=job_role_response['Role']['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ecs_role_policy = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Statement': [{'Sid': '',\n",
    "    'Effect': 'Allow',\n",
    "    'Principal': {'Service': 'ec2.amazonaws.com'},\n",
    "    'Action': 'sts:AssumeRole'}]}\n",
    "try:\n",
    "    instance_ecs_role = iam.create_role(\n",
    "        RoleName='ecsInstanceRole',\n",
    "        AssumeRolePolicyDocument=json.dumps(batch_ecs_role_policy),\n",
    "        MaxSessionDuration=3600)\n",
    "except ClientError as e:\n",
    "     instance_ecs_role = iam.get_role(RoleName='ecsInstanceRole')\n",
    "    \n",
    "# EC2 Conatiner Service\n",
    "iam.attach_role_policy(\n",
    "    RoleName=instance_ecs_role['Role']['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role')\n",
    "\n",
    "# S3 Access\n",
    "iam.attach_role_policy(\n",
    "    RoleName=instance_ecs_role['Role']['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conatiner_profile = iam.create_instance_profile(\n",
    "        InstanceProfileName='ecsInstanceProfile')\n",
    "except iam.exceptions.EntityAlreadyExistsException:\n",
    "    conatiner_profile = iam.get_instance_profile(\n",
    "        InstanceProfileName='ecsInstanceProfile')\n",
    "try:    \n",
    "    iam.add_role_to_instance_profile(\n",
    "        RoleName=instance_ecs_role['Role']['RoleName'],\n",
    "        InstanceProfileName=conatiner_profile['InstanceProfile']['InstanceProfileName'])\n",
    "except iam.exceptions.LimitExceededException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_service_policy = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Statement': [{'Sid': '',\n",
    "    'Effect': 'Allow',\n",
    "    'Principal': {'Service': 'batch.amazonaws.com'},\n",
    "    'Action': 'sts:AssumeRole'}]\n",
    "}\n",
    "\n",
    "try:\n",
    "    batch_service_role_response = iam.create_role(\n",
    "    RoleName='Batch_Service_Role',\n",
    "    AssumeRolePolicyDocument=json.dumps(batch_service_policy),\n",
    "    Description='IAM Role for Batch service',\n",
    "    MaxSessionDuration=3600)\n",
    "except ClientError as e:\n",
    "    batch_service_role_response = iam.get_role(RoleName='Batch_Service_Role')\n",
    "\n",
    "# Batch Service\n",
    "iam.attach_role_policy(\n",
    "    RoleName=batch_service_role_response['Role']['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom EC2 launch template\n",
    "Since the GAMOS Docker image is large, we need to increase the default storage space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_user_data = b\"\"\"Content-Type: multipart/mixed; boundary=\"==BOUNDARY==\"\n",
    "MIME-Version: 1.0\n",
    "\n",
    "--==BOUNDARY==\n",
    "Content-Type: text/cloud-boothook; charset=\"us-ascii\"\n",
    "\n",
    "# Set Docker daemon option dm.basesize so each container gets up to 50GB\n",
    "cloud-init-per once docker_options echo 'OPTIONS=\"${OPTIONS} --storage-opt dm.basesize=50GB\"' >> /etc/sysconfig/docker\n",
    "\n",
    "--==BOUNDARY==--\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_template_response = ec2_client.create_launch_template(\n",
    "    LaunchTemplateName='launch-template-batch-via-boto3-4',\n",
    "    LaunchTemplateData={\n",
    "        'EbsOptimized': False,\n",
    "        'BlockDeviceMappings':[{'DeviceName': '/dev/xvda',\n",
    "            'Ebs': {'Encrypted': False,\n",
    "             'DeleteOnTermination': True,\n",
    "             'VolumeSize': 16,\n",
    "             'VolumeType': 'gp2'}},\n",
    "           {'DeviceName': '/dev/xvdcz',\n",
    "            'Ebs': {'Encrypted': False,\n",
    "             'DeleteOnTermination': True,\n",
    "             'VolumeSize': 320,\n",
    "             'VolumeType': 'gp2'}}],\n",
    "#         'NetworkInterfaces': [{'AssociatePublicIpAddress': True,\n",
    "#             'DeleteOnTermination': True}],\n",
    "        'ImageId': 'ami-035a1bdaf0e4bf265', # This is an ECS optimized AMI\n",
    "        'KeyName': key_name,\n",
    "        'Monitoring': {'Enabled': True},\n",
    "        'DisableApiTermination': False,\n",
    "        'InstanceInitiatedShutdownBehavior': 'stop',\n",
    "        'UserData': base64.b64encode(custom_user_data).decode('UTF-8'), # This updates the space given to Docker\n",
    "        'SecurityGroupIds': [default_security_group_id, dartmouth_security_group_id ],\n",
    "        'TagSpecifications': [{\n",
    "            'ResourceType': 'instance',\n",
    "            'Tags': [{'Key': 'Name', 'Value': 'Docker_GAMOS boto3_LT'}]\n",
    "        }]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure compute environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_client = boto3.client('batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get all subnets based on the security groups and corresponding VPCs\n",
    "all_security_groups = ec2_client.describe_security_groups(GroupIds=[default_security_group_id,\n",
    "                                                                    dartmouth_security_group_id])\n",
    "all_vpc_ids = []\n",
    "for each_sg in all_security_groups['SecurityGroups']:\n",
    "    cur_id = each_sg['VpcId']\n",
    "    if cur_id not in all_vpc_ids:\n",
    "        all_vpc_ids.append(cur_id)\n",
    "\n",
    "all_subnets = ec2_client.describe_subnets(Filters=[{'Name':'vpc-id', 'Values': all_vpc_ids}])['Subnets']\n",
    "all_subnet_ids = []\n",
    "for each_subnet in all_subnets:\n",
    "    cur_subnet_id = each_subnet['SubnetId']\n",
    "    if cur_subnet_id not in all_subnet_ids:\n",
    "        all_subnet_ids.append(cur_subnet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the compute cluster\n",
    "compute_name = 'GAMOS_6_1_Cluster-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a compute cluster\n",
    "compute_env_response = batch_client.create_compute_environment(\n",
    "    computeEnvironmentName=compute_name,\n",
    "    type='MANAGED',\n",
    "    state='ENABLED',\n",
    "    computeResources={\n",
    "        'type': 'EC2',\n",
    "        'minvCpus': 0,\n",
    "        'maxvCpus': 128,\n",
    "        'desiredvCpus': 0,\n",
    "        'instanceTypes': [\n",
    "            'c5.2xlarge', 'c5.4xlarge', 'c5.9xlarge',\n",
    "        ],\n",
    "        'subnets': all_subnet_ids,\n",
    "        'securityGroupIds': [default_security_group_id, dartmouth_security_group_id],\n",
    "        'ec2KeyPair': key_name,\n",
    "        'instanceRole': conatiner_profile['InstanceProfile']['Arn'],\n",
    "        'tags': {\n",
    "            'Name': 'Batch GAMOS 6_1'\n",
    "        },\n",
    "        'launchTemplate': {\n",
    "            'launchTemplateId': launch_template_response['LaunchTemplate']['LaunchTemplateId']\n",
    "        },\n",
    "    },\n",
    "    serviceRole=batch_service_role_response['Role']['Arn']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a job queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_queue_name = 'GAMOS_6_1_queue-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    job_queue_response = batch_client.create_job_queue(\n",
    "        jobQueueName=job_queue_name,\n",
    "        state='ENABLED',\n",
    "        priority=80,\n",
    "        computeEnvironmentOrder=[\n",
    "            {\n",
    "                'order': 1,\n",
    "                'computeEnvironment': compute_env_response['computeEnvironmentName']\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "except ClientError as e:\n",
    "    job_queue_response = batch_client.describe_job_queues(jobQueues=[job_queue_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_queue_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and submit jobs\n",
    "The loop below will create 10 simulations, each with a different random seed. Each simulation will run for `number_events` events. The `inclusion_depth` alters the placement of the tumor inclusion by modifying the placement in the `world.geom` file. If multple depths are desired, the `inclusion_depth` value can be changed and cell block can be re-run. Every time the cell is run 10 more jobs will be added to the job queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6MV with fluence detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1000\n",
    "number_events = 1000000\n",
    "inclusion_depth = 7\n",
    "\n",
    "for i in range(0,10):\n",
    "    random_seed_str = str(random_seed+i) \n",
    "    \n",
    "    current_job_definition_name = \"GAMOSexample-6MV-fluence-tumor-{}mm-{}\".format('_'.join(str(inclusion_depth).split('.')), str(i))\n",
    "    \n",
    "    gamos_job_def = {\n",
    "        \"jobDefinitionName\": current_job_definition_name,\n",
    "        \"type\": \"container\",\n",
    "        \"parameters\": {},\n",
    "        \"retryStrategy\": {\n",
    "            \"attempts\": 1\n",
    "        },\n",
    "        \"containerProperties\": {\n",
    "            \"image\": ecr_repository['repository']['repositoryUri'],\n",
    "            \"vcpus\": 2,\n",
    "            \"memory\": 3000,\n",
    "            \"command\": [\n",
    "                \"batch_gamos.sh\",\n",
    "                str(random_seed_str),\n",
    "                str(inclusion_depth),\n",
    "                str(number_events),\n",
    "                \"transport.in\"\n",
    "            ],\n",
    "            \"jobRoleArn\": job_role_response['Role']['Arn'],\n",
    "            \"volumes\": [],\n",
    "            \"environment\": [\n",
    "                {\n",
    "                    \"name\": \"BATCH_FILE_S3_URL\",\n",
    "                    \"value\": \"s3://{}/{}.zip\".format(run_zip_name_f, run_zip_name_f)\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"BATCH_FILE_TYPE\",\n",
    "                    \"value\": \"zip\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"timeout\": {\n",
    "        \"attemptDurationSeconds\": 30000\n",
    "        }\n",
    "    }\n",
    "    # Bunlde job definition into specific job in queue\n",
    "    response = batch_client.register_job_definition(**gamos_job_def)\n",
    "    gamos_job = {\n",
    "        'jobName': \"JOB-{}\".format(current_job_definition_name),\n",
    "        'jobQueue': job_queue_response['jobQueueArn'],#job_queue_response['jobQueueArn'],\n",
    "        \"jobDefinition\": current_job_definition_name\n",
    "    }\n",
    "    # Submit job to Batch queue\n",
    "    response = batch_client.submit_job(**gamos_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6MV without fluence detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1000\n",
    "number_events = 1000000\n",
    "inclusion_depth = 7\n",
    "\n",
    "for i in range(0,10):\n",
    "    random_seed_str = str(random_seed+i) \n",
    "    \n",
    "    current_job_definition_name = \"GAMOSexample-6MV-nofluence-tumor-{}mm-{}\".format('_'.join(str(inclusion_depth).split('.')), str(i))\n",
    "    \n",
    "    gamos_job_def = {\n",
    "        \"jobDefinitionName\": current_job_definition_name,\n",
    "        \"type\": \"container\",\n",
    "        \"parameters\": {},\n",
    "        \"retryStrategy\": {\n",
    "            \"attempts\": 1\n",
    "        },\n",
    "        \"containerProperties\": {\n",
    "            \"image\": ecr_repository['repository']['repositoryUri'],\n",
    "            \"vcpus\": 2,\n",
    "            \"memory\": 3000,\n",
    "            \"command\": [\n",
    "                \"batch_gamos.sh\",\n",
    "                str(random_seed_str),\n",
    "                str(inclusion_depth),\n",
    "                str(number_events),\n",
    "                \"transport.in\"\n",
    "            ],\n",
    "            \"jobRoleArn\": job_role_response['Role']['Arn'],\n",
    "            \"volumes\": [],\n",
    "            \"environment\": [\n",
    "                {\n",
    "                    \"name\": \"BATCH_FILE_S3_URL\",\n",
    "                    \"value\": \"s3://{}/{}.zip\".format(run_zip_name_nf, run_zip_name_nf)\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"BATCH_FILE_TYPE\",\n",
    "                    \"value\": \"zip\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"timeout\": {\n",
    "        \"attemptDurationSeconds\": 30000\n",
    "        }\n",
    "    }\n",
    "    # Bunlde job definition into specific job in queue\n",
    "    response = batch_client.register_job_definition(**gamos_job_def)\n",
    "    gamos_job = {\n",
    "        'jobName': \"JOB-{}\".format(current_job_definition_name),\n",
    "        'jobQueue': job_queue_response['jobQueueArn'],#job_queue_response['jobQueueArn'],\n",
    "        \"jobDefinition\": current_job_definition_name\n",
    "    }\n",
    "    # Submit job to Batch queue\n",
    "    response = batch_client.submit_job(**gamos_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
