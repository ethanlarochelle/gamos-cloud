{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation analysis\n",
    "A subset of the analysis performed for the corresponding manuscript is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import boto3\n",
    "import shutil\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from scipy.interpolate import interp1d, spline\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../data/'\n",
    "output_dir = '../results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_dir_exists(path):\n",
    "    \"\"\"\n",
    "    Checks if directory tree in path exists. If not it created them.\n",
    "    :param path: the path to check if it exists\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "\n",
    "def download_dir(client, bucket, path, target):\n",
    "    \"\"\"\n",
    "    Downloads recursively the given S3 path to the target directory.\n",
    "    :param client: S3 client to use.\n",
    "    :param bucket: the name of the bucket to download from\n",
    "    :param path: The S3 directory to download.\n",
    "    :param target: the local directory to download the files to.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle missing / at end of prefix\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "\n",
    "    paginator = client.get_paginator('list_objects_v2')\n",
    "    for result in paginator.paginate(Bucket=bucket, Prefix=path):\n",
    "        # Download each file individually\n",
    "        for key in result['Contents']:\n",
    "            # Calculate relative path\n",
    "            rel_path = key['Key'][len(path):]\n",
    "            # Skip paths ending in /\n",
    "            if not key['Key'].endswith('/'):\n",
    "                local_file_path = os.path.join(target, rel_path)\n",
    "                # Make sure directories exist\n",
    "                local_file_dir = os.path.dirname(local_file_path)\n",
    "                assert_dir_exists(local_file_dir)\n",
    "                client.download_file(bucket, key['Key'], local_file_path)\n",
    "                \n",
    "                \n",
    "def fix_header_for_pandas(directory, file_prefix, suffix):\n",
    "    all_files = os.listdir(directory)\n",
    "    for each_file in all_files:\n",
    "        if (each_file.startswith(file_prefix)) and (each_file.endswith(suffix)):\n",
    "            full_file = os.path.join(directory, each_file)\n",
    "            from_file = open(full_file) \n",
    "            line = from_file.readline()\n",
    "            # Check is a header is provided\n",
    "            if line.startswith('HEADER'):\n",
    "                # Truncate header line to just have names of columns\n",
    "                revised_line = (',').join(line.split(',')[1:])\n",
    "                revised_file = open(os.path.join(directory, 'pdH_{}'.format(each_file)),mode=\"w\")\n",
    "                revised_file.write(revised_line)\n",
    "                shutil.copyfileobj(from_file, revised_file)\n",
    "                os.remove(full_file)\n",
    "                \n",
    "def makeLongId(cur_id, sim=0, seed=1000):\n",
    "    long_id = '{:04d}{:04d}{}'.format(sim, seed, cur_id)\n",
    "    return long_id\n",
    "\n",
    "def makeLongIdByRow(row, seed=1000):\n",
    "    if '.' in row['Depth']:\n",
    "        cur_depth = str(row['Depth'].replace('.', '00'))\n",
    "    else:\n",
    "        cur_depth = '{:02d}'.format(int(row['Depth']))\n",
    "    long_id = '{:04d}{}{}{}'.format(seed, cur_depth, int(row['EventID']), int(row['TrackID']))\n",
    "    return long_id\n",
    "    \n",
    "def readGamosOutputFile(directory, file_prefix, suffix, headers=[], chunksize=10**6):\n",
    "    list_ = []\n",
    "    all_files = os.listdir(directory)\n",
    "    for each_file in all_files:\n",
    "        if (each_file.startswith(file_prefix)) and (each_file.endswith(suffix)):\n",
    "            full_file = os.path.join(directory, each_file)\n",
    "            if headers==[]:\n",
    "                for chunk in pandas.read_csv(full_file, delimiter=',', header=0, iterator=True, chunksize=chunksize):\n",
    "                    list_.append(chunk)\n",
    "            else:\n",
    "                # Use custom headers\n",
    "                for chunk in pandas.read_csv(full_file, delimiter=',', names=headers, iterator=True, chunksize=chunksize):\n",
    "                    list_.append(chunk)\n",
    "    df = pandas.concat(list_)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def readMultiGamosOutputFiles(directory, file_prefix, headers=[], chunksize=10**6):\n",
    "    df = pandas.DataFrame()\n",
    "    dirs = []\n",
    "    all_contents = os.listdir(directory)\n",
    "    for each_entry in all_contents:\n",
    "        if os.path.isdir(os.path.join(directory, each_entry)):\n",
    "            dirs.append(each_entry)\n",
    "    if len(dirs)>0:\n",
    "        for each_dir in dirs:\n",
    "            rand_seed = each_dir.split('_')[-1]\n",
    "            depth = each_dir.split('_')[-2]\n",
    "            suffix = '{}_{}'.format(depth, rand_seed)\n",
    "#             print(suffix)\n",
    "            tmp_df = readGamosOutputFile(os.path.join(directory, each_dir), file_prefix, suffix)\n",
    "            if tmp_df.empty:\n",
    "#                 print('\\tempty')\n",
    "                df=df\n",
    "            else:\n",
    "                tmp_df['Depth'] = str(depth)\n",
    "                tmp_df['Seed'] = int(rand_seed)\n",
    "                tmp_df['LongID'] = tmp_df.apply(makeLongIdByRow, args=[int(rand_seed)], axis=1)\n",
    "                df = df.append(tmp_df)\n",
    "#                 print(len(df))\n",
    "    else:\n",
    "        df = readGamosOutputFile(directory, suffix, headers, chunksize)\n",
    "    return df\n",
    "\n",
    "def readGamosFluenceFile(file, readInError=False, divide_by_sum=False):\n",
    "    new_file = \"{}.new\".format(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        file_contents = f.read()\n",
    "        new_index = 0\n",
    "        i = 0\n",
    "        while i<5:\n",
    "            new_index = file_contents.find(\"\\n\", new_index+1)\n",
    "            i +=1\n",
    "    with open(new_file, \"w\") as f:\n",
    "        f.write(file_contents[new_index:])\n",
    "    if readInError:\n",
    "        df = pandas.read_csv(new_file, delimiter=',', names=['voxel','fluence', 'error(rel)', 'sumV2'])   \n",
    "    else:\n",
    "        df = pandas.read_csv(new_file, delimiter=',', names=['voxel','fluence'])\n",
    "    if divide_by_sum:\n",
    "        #print(df.tail(1)['voxel'].values[0].split(' ')[1])\n",
    "        total_sum = float(df.tail(1)['voxel'].values[0].split(' ')[1])\n",
    "        df = df[:-1]\n",
    "        new_fluence_values = df['fluence']/total_sum\n",
    "        df['fluence'] = new_fluence_values\n",
    "    else:\n",
    "        df = df[:-1]\n",
    "    return df\n",
    "\n",
    "def readGamosDoseFile(file):\n",
    "    new_file = \"{}.new\".format(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        file_contents = f.read()\n",
    "        new_index = 0\n",
    "        i = 0\n",
    "        while i<5:\n",
    "            new_index = file_contents.find(\"\\n\", new_index+1)\n",
    "            i +=1\n",
    "    with open(new_file, \"w\") as f:\n",
    "        f.write(file_contents[new_index:])\n",
    "    df = pandas.read_csv(new_file, delimiter=',', names=['voxel','dose', 'rel_error', 'sumV2'])\n",
    "    df = df[:-1]\n",
    "    return df\n",
    "\n",
    "def moving_average(a, n=10) :\n",
    "    ret = numpy.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def integrate_spectrum(amplitudes, wavelengths, a, b):\n",
    "    \"\"\"Integrate a spectrum from a to b\n",
    "    :param amplitudes: An array of length N containing amplitudes for a given spectrum\n",
    "    :param wavelengths: An array of length N containing wavelengths whigh correspond \n",
    "        to the amplitudes for a given spectrum\n",
    "    :param a: The starting wavelength for integration of the spectrum\n",
    "    :param b: The ending wavelength for integration of the spectrum\n",
    "    :returns: The area under the spectrum using composite trapezoidal rule\n",
    "    \"\"\"\n",
    "    #Find subset of array where spectrum is between a and b\n",
    "    #num_measurements = amplitudes.shape[0][0]/amplitudes.shape[0][1]\n",
    "    spec_roi = amplitudes[numpy.where((wavelengths>=a) & (wavelengths<=b))]\n",
    "    wv_roi = wavelengths[numpy.where((wavelengths>=a) & (wavelengths<=b))]\n",
    "    dx = (wv_roi[1] - wv_roi[0])\n",
    "    area = numpy.trapz(spec_roi, wv_roi, dx=dx)\n",
    "        \n",
    "    return area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download simulation output from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_6mv_name = 'gamos-ptg4-6mv-nofluence'\n",
    "root_path_name = '6MV_NF'\n",
    "s3_paths = []\n",
    "# S3 folders were created programatically to have a structured name\n",
    "for depth in [0,1,2,3,5,7]:\n",
    "    for i in range(0,10):\n",
    "        random_seed = 1000+i\n",
    "        cur_path_name = '{}_{}_{}'.format(root_path_name, depth, random_seed)\n",
    "        s3_paths.append(cur_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using list of S3 folders in bucket, copy each to local system\n",
    "for each_dir in s3_paths:\n",
    "    new_dir = os.path.join(output_dir, 'simulations', root_path_name , each_dir)\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    download_dir(s3_client, s3_bucket_6mv_name ,  each_dir, new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify all the detector files with headers to make it easier for pandas to parse\n",
    "for each_dir in s3_paths:\n",
    "    sim_dir = os.path.join(output_dir, 'simulations', root_path_name, each_dir)\n",
    "    fix_header_for_pandas(sim_dir, 'Detect_', each_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluorescence at surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_at_surface = readMultiGamosOutputFiles(os.path.join(output_dir, 'simulations', root_path_name),\n",
    "                                          'pdH_Detect_fl_at_surface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_fl = {}\n",
    "total_str = ''\n",
    "for each_depth in ['0', '1', '2', '3', '5', '7']:\n",
    "    surface_fl[depth] = plt.hist2d(fl_at_surface[fl_at_surface['Depth']==each_depth]['FinalPosX'],\n",
    "               fl_at_surface[fl_at_surface['Depth']==each_depth]['FinalPosY'],\n",
    "               numpy.arange(-30,30))\n",
    "    center = surface_fl[depth][0][29:32, 29:32]\n",
    "    max_center = numpy.nanmax(center)\n",
    "    cur_avg = numpy.nanmean(surface_fl[depth][0][surface_fl[depth][0]>(0.5*max_center)])\n",
    "    fwhm_count = numpy.sum(surface_fl[depth][0]>(0.5*max_center))\n",
    "    cur_fwhm = 2*numpy.sqrt(fwhm_count/numpy.pi)\n",
    "    new_line = '{}mm:\\tFWHM:{:1.2f}mm,\\tMax:{:1.0f},\\tMean:{:1.1f}'.format(each_depth, cur_fwhm, max_center, cur_avg)\n",
    "    print(new_line)\n",
    "    total_str=total_str+new_line+'\\n'\n",
    "with open(os.path.join(output_dir, '6MV_fluorescence_FWHM.txt'), \"w\") as text_file:\n",
    "    print(total_str, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,8])\n",
    "hist2d_6MV_7 = plt.hist2d(fl_at_surface[fl_at_surface['Depth']=='2']['FinalPosX'],\n",
    "               fl_at_surface[fl_at_surface['Depth']=='2']['FinalPosY'],\n",
    "               numpy.arange(-30,30))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluoresence starting in tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_starting_in_tumor = readMultiGamosOutputFiles(os.path.join(output_dir, 'simulations', root_path_name),\n",
    "                                          'pdH_Detect_fl_start_tumor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluoresence exiting tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_exiting_tumor = readMultiGamosOutputFiles(os.path.join(output_dir, 'simulations', root_path_name),\n",
    "                                          'pdH_Detect_fl_exit_tumor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluoresence attenuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fl_starting_in_tumor[fl_starting_in_tumor['Depth']=='7']['WavelengthEnergy*nm']\n",
    "# fl_exiting_tumor[fl_exiting_tumor['Depth']=='7']['WavelengthEnergy*nm']\n",
    "# fl_at_surface[fl_at_surface['Depth']=='7']['WavelengthEnergy*nm']\n",
    "total_str = ''\n",
    "for depth in ['0','1','2','3','5','7']:\n",
    "    starting_fl_count = len(fl_starting_in_tumor[fl_starting_in_tumor['Depth']==depth]['WavelengthEnergy*nm'])\n",
    "    exiting_fl_count = len(fl_exiting_tumor[fl_exiting_tumor['Depth']==depth]['WavelengthEnergy*nm'])\n",
    "    surface_fl_count = len(fl_at_surface[fl_at_surface['Depth']==depth]['WavelengthEnergy*nm'])\n",
    "    percent_exiting = exiting_fl_count/starting_fl_count\n",
    "    percent_surface = surface_fl_count/starting_fl_count\n",
    "    new_line = '{}mm:\\t(Starting N:{}),\\tExiting: {:1.2f}%,\\tSurface: {:1.2f}% ({})'.format(depth, starting_fl_count, percent_exiting*100, percent_surface*100, surface_fl_count)\n",
    "    print(new_line)\n",
    "    total_str=total_str+new_line+'\\n'\n",
    "with open(os.path.join(output_dir, '6MV_fluorescence_tissue_filter.txt'), \"w\") as text_file:\n",
    "    print(total_str, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherenkov at surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherenkov_at_surface = readMultiGamosOutputFiles(os.path.join(output_dir, 'simulations', root_path_name),\n",
    "                                          'pdH_Detect_Cerenkov_at_surface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_ch_0 = plt.hist(cherenkov_at_surface[cherenkov_at_surface['Depth']=='0']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# surface_ch_1 = plt.hist(cherenkov_at_surface[cherenkov_at_surface['Depth']=='1']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# surface_ch_2 = plt.hist(cherenkov_at_surface[cherenkov_at_surface['Depth']=='2']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# surface_ch_3 = plt.hist(cherenkov_at_surface[cherenkov_at_surface['Depth']=='3']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "surface_ch_5 = plt.hist(cherenkov_at_surface[cherenkov_at_surface['Depth']=='5']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# surface_ch_7 = plt.hist(cherenkov_at_surface[cherenkov_at_surface['Depth']=='7']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# surface_ch_10 = plt.hist(cherenkov_at_surface[cherenkov_at_surface['Depth']=='10']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "plt.show()\n",
    "\n",
    "mean_surface_ch_hist_freq = numpy.nanmean([surface_ch_0[0], surface_ch_5[0]], axis=0)\n",
    "#                                            surface_ch_1[0],surface_ch_2[0],\n",
    "#                                            surface_ch_3[0], surface_ch_5[0], surface_ch_7[0],\n",
    "#                                            surface_ch_10[0]], axis=0)\n",
    "max_surface_ch_hist_freq = numpy.nanmax(mean_surface_ch_hist_freq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_ch_df = pandas.DataFrame()\n",
    "surf_ch_df['wavelength'] = center\n",
    "surf_ch_df['photons_0mm'] = surface_ch_0[0]\n",
    "# surf_ch_df['photons_1mm'] = surface_ch_1[0]\n",
    "# surf_ch_df['photons_2mm'] = surface_ch_2[0]\n",
    "# surf_ch_df['photons_3mm'] = surface_ch_3[0]\n",
    "surf_ch_df['photons_5mm'] = surface_ch_5[0]\n",
    "# surf_ch_df['photons_7mm'] = surface_ch_7[0]\n",
    "# surf_ch_df['photons_10mm'] = surface_ch_10[0]\n",
    "\n",
    "# Write fluoresence emission starting info\n",
    "writer = pandas.ExcelWriter(os.path.join(output_dir, 'Cherenkov_emissions.xlsx'))\n",
    "surf_ch_df.to_excel(writer, 'Surface')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherenkov exiting tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherenkov_exit_tumor = readMultiGamosOutputFiles(os.path.join(output_dir, 'simulations', root_path_name),\n",
    "                                          'pdH_Detect_Cerenkov_exit_tumor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_ch_0 = plt.hist(cherenkov_exit_tumor[cherenkov_exit_tumor['Depth']=='0']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# exit_ch_1 = plt.hist(cherenkov_exit_tumor[cherenkov_exit_tumor['Depth']=='1']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# exit_ch_2 = plt.hist(cherenkov_exit_tumor[cherenkov_exit_tumor['Depth']=='2']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# exit_ch_3 = plt.hist(cherenkov_exit_tumor[cherenkov_exit_tumor['Depth']=='3']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "exit_ch_5 = plt.hist(cherenkov_exit_tumor[cherenkov_exit_tumor['Depth']=='5']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "# exit_ch_7 = plt.hist(cherenkov_exit_tumor[cherenkov_exit_tumor['Depth']=='7']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "plt.show()\n",
    "\n",
    "mean_exit_ch_hist_freq = numpy.nanmean([exit_ch_0[0], exit_ch_5[0]],axis=0)\n",
    "#                                         exit_ch_1[0],exit_ch_2[0],\n",
    "#                                         exit_ch_3[0], exit_ch_5[0], exit_ch_7[0],\n",
    "#                                         exit_ch_10[0]], axis=0)\n",
    "max_exit_ch_hist_freq = numpy.nanmax(mean_exit_ch_hist_freq )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_ch_df = pandas.DataFrame()\n",
    "exit_ch_df['wavelength'] = center\n",
    "exit_ch_df['photons_0mm'] = exit_ch_0[0]\n",
    "# exit_ch_df['photons_1mm'] = exit_ch_1[0]\n",
    "# exit_ch_df['photons_2mm'] = exit_ch_2[0]\n",
    "# exit_ch_df['photons_3mm'] = exit_ch_3[0]\n",
    "exit_ch_df['photons_5mm'] = exit_ch_5[0]\n",
    "# exit_ch_df['photons_7mm'] = exit_ch_7[0]\n",
    "# exit_ch_df['photons_10mm'] = exit_ch_10[0]\n",
    "\n",
    "# Write fluoresence emission starting info\n",
    "# writer = pandas.ExcelWriter(os.path.join(output_dir, 'Cherenkov_emissions.xlsx'))\n",
    "exit_ch_df.to_excel(writer, 'Exit')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherenkov in tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ch_df = pandas.DataFrame()\n",
    "\n",
    "for depth in [0,1,2,3,5,7,10]:\n",
    "    for i in range(0,10):\n",
    "        random_seed = 1000+i\n",
    "        cur_suffix = '{}_{}_{}'.format(root_path_name, depth, random_seed)\n",
    "#         print(cur_suffix)\n",
    "        cur_dir = os.path.join(output_dir, 'simulations', root_path_name, cur_suffix)\n",
    "        tmp_ch_df = readGamosOutputFile(cur_dir,\n",
    "                            'pdH_Detect_Cerenkov_in_tumor',\n",
    "                            cur_suffix)\n",
    "        grouped_df = tmp_ch_df.groupby(['EventID', 'TrackID'], sort=False, as_index=False)[\"WavelengthEnergy*nm\"].first()\n",
    "        \n",
    "        grouped_df['Depth'] = depth\n",
    "    all_ch_df = all_ch_df.append(grouped_df)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_in_tumor_0 = plt.hist(all_ch_df[all_ch_df['Depth']=='0']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "ch_in_tumor_1 = plt.hist(all_ch_df[all_ch_df['Depth']=='1']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "ch_in_tumor_2 = plt.hist(all_ch_df[all_ch_df['Depth']=='2']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "ch_in_tumor_3 = plt.hist(all_ch_df[all_ch_df['Depth']=='3']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "ch_in_tumor_5 = plt.hist(all_ch_df[all_ch_df['Depth']=='5']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n",
    "ch_in_tumor_7 = plt.hist(all_ch_df[all_ch_df['Depth']=='7']['WavelengthEnergy*nm'],numpy.linspace(350,900, 111))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tumor_ch_df = pandas.DataFrame()\n",
    "in_tumor_ch_df['wavelength'] = center\n",
    "in_tumor_ch_df['photons_0mm'] = ch_in_tumor_0[0]\n",
    "in_tumor_ch_df['photons_1mm'] = ch_in_tumor_1[0]\n",
    "in_tumor_ch_df['photons_2mm'] = ch_in_tumor_2[0]\n",
    "in_tumor_ch_df['photons_3mm'] = ch_in_tumor_3[0]\n",
    "in_tumor_ch_df['photons_5mm'] = ch_in_tumor_5[0]\n",
    "in_tumor_ch_df['photons_7mm'] = ch_in_tumor_7[0]\n",
    "in_tumor_ch_df['photons_10mm'] = ch_in_tumor_10[0]\n",
    "\n",
    "# Write fluoresence emission starting info\n",
    "# writer = pandas.ExcelWriter(os.path.join(output_dir, 'Cherenkov_emissions.xlsx'))\n",
    "in_tumor_ch_df.to_excel(writer, 'In_tumor')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluorescence excitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_suffix in s3_paths:\n",
    "    print('{}'.format(cur_suffix))\n",
    "    print('\\tReading input files...')\n",
    "    secondaries_potential_exc = readGamosOutputFile(os.path.join(output_dir, 'simulations', root_path_name, cur_suffix),\n",
    "                                      'pdH_Detect_fl_sec_excitation_in_tumor',\n",
    "                                      cur_suffix)\n",
    "\n",
    "    fl_start = readGamosOutputFile(os.path.join(output_dir, 'simulations', root_path_name, cur_suffix),\n",
    "                                      'pdH_Detect_fl_start_tumor',\n",
    "                                      cur_suffix)\n",
    "    matching_events = secondaries_potential_exc[secondaries_potential_exc['EventID'].isin(fl_start['EventID'])]\n",
    "    # Parse out excitation and emission info\n",
    "    unique_events = fl_start['EventID'].unique()\n",
    "    matching_tracks_df = pandas.DataFrame()\n",
    "    valid_fl_events_df = pandas.DataFrame()\n",
    "    print('\\tParsing emission and excitation...')\n",
    "    for each_event in unique_events:\n",
    "        # Analyze one event\n",
    "        cur_fl_slice = fl_start[fl_start['EventID']==each_event]\n",
    "        # Find tracks that are not sequential\n",
    "        valid_tracks = numpy.insert(True, 1, numpy.diff(cur_fl_slice['TrackID'])>1)\n",
    "        cur_valid_tracks = cur_fl_slice[valid_tracks]\n",
    "        # Find exciation event based on track number\n",
    "        tmp_df = matching_events[(matching_events['EventID']==each_event) & \n",
    "                                 (matching_events['TrackID']+1).isin(cur_valid_tracks['TrackID'])]\n",
    "        # Append to data frame\n",
    "        matching_tracks_df = matching_tracks_df.append(tmp_df)\n",
    "        valid_fl_events_df = valid_fl_events_df.append(cur_valid_tracks)\n",
    "\n",
    "    print('\\tWriting outputs...')\n",
    "    # Write fluoresence emission starting info\n",
    "    writer = pandas.ExcelWriter(os.path.join(output_dir, 'simulations', root_path_name, cur_suffix,\n",
    "                                             'pandas_fl_emission_start_tumor_{}.xlsx'.format(cur_suffix)))\n",
    "    valid_fl_events_df.to_excel(writer, '{}'.format(cur_suffix))\n",
    "    writer.save()\n",
    "\n",
    "    # Write fluoresence excitation info\n",
    "    writer = pandas.ExcelWriter(os.path.join(output_dir, 'simulations', root_path_name, cur_suffix,\n",
    "                                             'pandas_fl_excitation_start_tumor_{}.xlsx'.format(cur_suffix)))\n",
    "    matching_tracks_df.to_excel(writer, '{}'.format(cur_suffix))\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all Excel excitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_excitation_df = pandas.DataFrame()\n",
    "fl_emission_df = pandas.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for depth in [0,1,2,3,5,7,10]:\n",
    "    for i in range(0,10):\n",
    "        random_seed = 1000+i\n",
    "        cur_suffix = '{}_{}_{}'.format(root_path_name, depth, random_seed)\n",
    "        print(cur_suffix)\n",
    "        tmp_exc_df = pandas.read_excel(os.path.join(output_dir, 'simulations', root_path_name, cur_suffix,\n",
    "                                                 'pandas_fl_excitation_start_tumor_{}.xlsx'.format(cur_suffix)))\n",
    "        tmp_exc_df['Depth'] = depth\n",
    "        all_excitation_df = all_excitation_df.append(tmp_exc_df)\n",
    "        \n",
    "        tmp_ems_df = pandas.read_excel(os.path.join(output_dir, 'simulations', root_path_name, cur_suffix,\n",
    "                                                 'pandas_fl_emission_start_tumor_{}.xlsx'.format(cur_suffix)))\n",
    "        tmp_ems_df['Depth'] = depth\n",
    "        fl_emission_df = fl_emission_df.append(tmp_ems_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_exc_0 = plt.hist(all_excitation_df[all_excitation_df['Depth']==0]['WavelengthEnergy*nm'],numpy.linspace(350,700, 71))\n",
    "fl_exc_1 = plt.hist(all_excitation_df[all_excitation_df['Depth']==1]['WavelengthEnergy*nm'],numpy.linspace(350,700, 71))\n",
    "fl_exc_2 = plt.hist(all_excitation_df[all_excitation_df['Depth']==2]['WavelengthEnergy*nm'],numpy.linspace(350,700, 71))\n",
    "fl_exc_3 = plt.hist(all_excitation_df[all_excitation_df['Depth']==3]['WavelengthEnergy*nm'],numpy.linspace(350,700, 71))\n",
    "fl_exc_5 = plt.hist(all_excitation_df[all_excitation_df['Depth']==5]['WavelengthEnergy*nm'],numpy.linspace(350,700, 71))\n",
    "fl_exc_7 = plt.hist(all_excitation_df[all_excitation_df['Depth']==7]['WavelengthEnergy*nm'],numpy.linspace(350,700, 71))\n",
    "fl_exc_10 = plt.hist(all_excitation_df[all_excitation_df['Depth']==10]['WavelengthEnergy*nm'],numpy.linspace(350,700, 71))\n",
    "width = 0.9 * 5\n",
    "center = (new_depth_1[1][:-1] + new_depth_1[1][1:]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excite_ch_df = pandas.DataFrame()\n",
    "excite_ch_df['wavelength'] = center\n",
    "excite_ch_df['photons_0mm'] = fl_exc_0[0]\n",
    "excite_ch_df['photons_1mm'] = fl_exc_1[0]\n",
    "excite_ch_df['photons_2mm'] = fl_exc_2[0]\n",
    "excite_ch_df['photons_3mm'] = fl_exc_3[0]\n",
    "excite_ch_df['photons_5mm'] = fl_exc_5[0]\n",
    "excite_ch_df['photons_7mm'] = fl_exc_7[0]\n",
    "excite_ch_df['photons_10mm'] = fl_exc_10[0]\n",
    "\n",
    "# Write fluoresence emission starting info\n",
    "# writer = pandas.ExcelWriter(os.path.join(output_dir, 'Cherenkov_emissions.xlsx'))\n",
    "excite_ch_df.to_excel(writer, 'Excitation')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Cherenkov wavelength distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_all = (ch_in_tumor_0[1][:-1] + ch_in_tumor_0[1][1:]) / 2\n",
    "bin_width_all = ch_in_tumor_0[1][1]-ch_in_tumor_0[1][0]\n",
    "center_fl = (fl_exc_1[1][:-1] + fl_exc_1[1][1:]) / 2\n",
    "bin_width_fl = fl_exc_1[1][1] - fl_exc_1[1][0]\n",
    "window=5\n",
    "\n",
    "plt.figure(figsize=[12,12])\n",
    "in_scale = integrate_spectrum(ch_in_tumor_2[0], center_all, 350,900)\n",
    "smooth_in = moving_average(100*ch_in_tumor_2[0]/in_scale,window)\n",
    "plt.plot(moving_average(center_all,window), smooth_in, label='In', lw=6, c='royalblue')\n",
    "\n",
    "\n",
    "exit_scale = integrate_spectrum(exit_ch_2[0], center_all, 350,900)\n",
    "smooth_exit = moving_average(100*exit_ch_2[0]/exit_scale,window)\n",
    "plt.plot(moving_average(center_all,window), smooth_exit, label='Exit', lw=6, c='darkgoldenrod')\n",
    "\n",
    "surface_scale = integrate_spectrum(surface_ch_2[0], center_all, 350,900)\n",
    "smooth_surface = moving_average(100*surface_ch_2[0]/surface_scale,window)\n",
    "plt.plot(moving_average(center_all,window), smooth_surface, label='Surface', lw=6, c='crimson')\n",
    "\n",
    "fl_exc_scale = integrate_spectrum(new_depth_2[0], center_fl, 350,700)\n",
    "smooth_exc = moving_average(100*new_depth_2[0]/fl_exc_scale,window)\n",
    "plt.plot(moving_average(center_fl,window), smooth_exc, label='Excitation', lw=6, ls=':', c='midnightblue')\n",
    "plt.legend(bbox_to_anchor=(0.5,-0.33), loc=\"lower center\", ncol=2)\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Normalized Probability (%)')\n",
    "plt.xlim(360,890)\n",
    "plt.ylim(0,0.6)\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(output_dir, 'Cherenkov_probabilities_2mm.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Tissue Optical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_file = os.path.join(input_dir, 'PtG4_MC_geometry_LaRochelle.xlsx')\n",
    "xl = pandas.ExcelFile(to_file)\n",
    "all_layers = {}\n",
    "layer_names = ['skin1', 'skin2', 'skin3',\n",
    "               'skin4', 'skin5', 'skin6',\n",
    "               'skin7', 'fat', 'muscle', 'tumor']\n",
    "for each_layer in layer_names:\n",
    "    if each_layer=='tumor':\n",
    "        all_layers[each_layer] = xl.parse(each_layer, skiprows=8)\n",
    "    else:\n",
    "        all_layers[each_layer] = xl.parse(each_layer, skiprows=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_of_interest = 'mu_eff' # Options: 'mu_eff' | 'mu_s_prime' | 'mu_a' | 'n' | 'g'\n",
    "\n",
    "all_styles = ['-', '--', '-.', ':']\n",
    "combined_layers = ['epidermis', 'dermis', 'blood-net', 'adipose', 'muscle', 'tumor']\n",
    "layer_map = [[0,1], [2,4], [3,5], [6,7], [8], [9]]\n",
    "# define the colormap\n",
    "cmap = plt.cm.inferno\n",
    "# extract all colors from the .viridis map\n",
    "cmap_array = numpy.array([cmap(i) for i in range(cmap.N)])\n",
    "num_layers = len(combined_layers)\n",
    "layer_colors = cmap_array[0::int(cmap.N/num_layers)]\n",
    "color_cycler = cycle(layer_colors)\n",
    "style_cycler = cycle(all_styles)\n",
    "              \n",
    "\n",
    "plt_yy = {}\n",
    "fig = plt.figure(figsize=[20,15])\n",
    "ax = fig.gca()\n",
    "n = 0\n",
    "for each_layer in combined_layers:\n",
    "    cur_color = next(color_cycler)\n",
    "    cur_style = next(style_cycler)\n",
    "    m=0\n",
    "    for layer_index in layer_map[n]:\n",
    "        sub_layer = layer_names[layer_index]\n",
    "        if sub_layer=='tumor':\n",
    "            fl_abs = all_layers[sub_layer]['mpt_fl_abs'].values\n",
    "            fl_ems = all_layers[sub_layer]['mpt_fl_ems'].values\n",
    "        if m==0:\n",
    "            cur_wavelengths = all_layers[sub_layer]['mpt_wavelength'].values\n",
    "            cur_mu_s = all_layers[sub_layer]['mpt_miescat'].values\n",
    "            cur_mu_a = all_layers[sub_layer]['mpt_abs'].values\n",
    "            cur_n = all_layers[sub_layer]['mpt_rindex'].values\n",
    "            cur_g = all_layers[sub_layer]['mpt_g'].values\n",
    "        else:\n",
    "            new_wv = all_layers[sub_layer]['mpt_wavelength'].values\n",
    "            cur_wavelengths = numpy.nanmean([cur_wavelengths,new_wv], axis=0)\n",
    "            new_mu_s = all_layers[sub_layer]['mpt_miescat'].values\n",
    "            cur_mu_s = numpy.nanmean([cur_mu_s, new_mu_s], axis=0)\n",
    "            new_mu_a = all_layers[sub_layer]['mpt_abs'].values\n",
    "            cur_mu_a = numpy.nanmean([cur_mu_a, new_mu_a], axis=0)\n",
    "            new_n = all_layers[sub_layer]['mpt_rindex'].values\n",
    "            cur_n = numpy.nanmean([cur_n, new_n],axis=0)\n",
    "            new_g = all_layers[sub_layer]['mpt_g'].values\n",
    "            cur_g = numpy.nanmean([cur_g,new_g], axis=0)\n",
    "            \n",
    "        m+=1\n",
    "\n",
    "    f_cubic_mu_a = interp1d(cur_wavelengths, cur_mu_a, kind='cubic')\n",
    "    f_cubic_mu_s = interp1d(cur_wavelengths, cur_mu_s, kind='cubic')\n",
    "    f_n = interp1d(cur_wavelengths, cur_n, kind='cubic')\n",
    "    f_g = interp1d(cur_wavelengths, cur_g, kind='cubic')\n",
    "    xx = numpy.linspace(cur_wavelengths.min(),cur_wavelengths.max(),550)\n",
    "    if len(layer_map[n])>1:\n",
    "        cur_label = r'$\\overline{%s}$'% each_layer\n",
    "    else:\n",
    "        cur_label = '${}$'.format(each_layer)\n",
    "    mu_eff = numpy.sqrt(3*f_cubic_mu_a(xx)*(f_cubic_mu_a(xx)+f_cubic_mu_s(xx)*(1-f_g(xx))))\n",
    "    plt_yy['mu_eff'] = mu_eff\n",
    "    plt_yy['mu_s_prime'] = f_cubic_mu_s(xx)*(1-f_g(xx))\n",
    "    plt_yy['mu_a'] = f_cubic_mu_a(xx)\n",
    "    plt_yy['n'] = f_n(xx)\n",
    "    plt_yy['g'] = f_g(xx)\n",
    "    if (attribute_of_interest=='n') or (attribute_of_interest=='g'):\n",
    "        plt.plot(xx, plt_yy[attribute_of_interest], color=cur_color, lw=10, label=cur_label, alpha=0.8, ls=cur_style)\n",
    "    else:\n",
    "        plt.semilogy(xx, plt_yy[attribute_of_interest], color=cur_color, lw=10, label=cur_label, alpha=0.8, ls=cur_style)\n",
    "\n",
    "    n+=1\n",
    "#plt.ylim(0.01, 50)\n",
    "plt.xlim(350, 900)\n",
    "plt.grid(True, axis='y', which='major', color='black', linestyle='-', lw=2, alpha=0.5)\n",
    "plt.grid(True, axis='y', which='minor', color='silver', linestyle=':', lw=1, alpha=0.5)\n",
    "if attribute_of_interest=='mu_eff':\n",
    "    plt.ylabel(\"$\\mu_{eff}$ (mm$^{-1}$)\", fontsize=55)\n",
    "elif attribute_of_interest=='mu_s_prime':\n",
    "    plt.ylabel(\"$\\mu_s^{\\'}$ (mm$^{-1}$)\", fontsize=55)\n",
    "elif attribute_of_interest=='mu_a':\n",
    "    plt.ylabel(\"$\\mu_a$ (mm$^{-1}$)\", fontsize=55)\n",
    "elif attribute_of_interest=='n':\n",
    "    plt.ylabel('n', fontsize=55)\n",
    "elif attribute_of_interest=='g':\n",
    " plt.ylabel('g', fontsize=55)\n",
    "\n",
    "ax.yaxis.set_tick_params(labelsize=40)\n",
    "\n",
    "plt.xlabel('Wavelength (nm)', fontsize=55)\n",
    "ax.xaxis.set_tick_params(labelsize=40, rotation=-45)\n",
    "\n",
    "plt.legend(loc='best', fontsize=45)\n",
    "plt.savefig(os.path.join(output_dir, 'All_{}.png'.format(attribute_of_interest)),bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[20,15])\n",
    "ax = fig.gca()\n",
    "f_fl_abs = interp1d(cur_wavelengths, fl_abs, kind='cubic')\n",
    "f_fl_ems = interp1d(cur_wavelengths, fl_ems, kind='cubic')\n",
    "plt.semilogy(xx,f_fl_abs(xx), color='indigo', lw=10, label='Fluorescence absorption', alpha=0.8, ls='--')\n",
    "#plt.semilogy(cur_wavelengths, fl_abs,color='indigo', lw=10, label='Fluorescence absorption', alpha=0.8, ls='--')\n",
    "plt.xlim(350, 900)\n",
    "plt.grid(True, axis='y', which='major', color='black', linestyle='-', lw=2, alpha=0.5)\n",
    "plt.grid(True, axis='y', which='minor', color='silver', linestyle=':', lw=1, alpha=0.5)\n",
    "plt.ylabel(\"Absorption (mm$^{-1}$)\", fontsize=55, color='indigo')\n",
    "plt.ylim([0.01,1])\n",
    "ax.yaxis.set_tick_params(labelsize=40, color='indigo')\n",
    "plt.xlabel('Wavelength (nm)', fontsize=55)\n",
    "ax.xaxis.set_tick_params(labelsize=40, rotation=-45)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(xx,f_fl_ems(xx), color='firebrick', lw=10, label='Fluorescence absorption', alpha=0.8, ls=':')\n",
    "ax2.yaxis.set_tick_params(labelsize=40, color='firebrick')\n",
    "ax2.set_ylim([0,1])\n",
    "ax2.set_ylabel(\"Normalized Emission\", fontsize=55, color='firebrick')\n",
    "plt.savefig(os.path.join(output_dir, 'PtG4_equiv_spectra_50uM.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "# plt.legend(loc='best', fontsize=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
